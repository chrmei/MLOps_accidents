# üö¶ MLOps Road Accident Prediction

A **containerized** MLOps project for predicting road accidents using machine learning. This project implements a complete MLOps workflow from data ingestion to model serving, with a focus on **reproducibility, versioning, and containerization**.

## üìã Table of Contents

- [Project Overview](#project-overview)
- [üê≥ Docker & Containerization](#-docker--containerization) ‚≠ê **Start Here**
- [Pipeline Overview](#pipeline-overview)
- [Current State](#current-state)
- [Getting Started](#getting-started)
- [Project Structure](#project-structure)
- [Technology Stack](#technology-stack)
- [Workflow](#workflow)
- [MLflow Model Registry](#-mlflow-model-registry)
- [Multi-Model Training Framework](#-multi-model-training-framework)
- [Team Structure](#-team-structure)
- [Roadmap](#-roadmap)
- [Contributing](#contributing)
- [License](#license)

## üéØ Project Overview

This project is an MLOps implementation for road accident prediction, designed as a learning and production-ready template. The system processes road accident data from multiple sources, trains machine learning models to predict accident severity, and serves predictions through a REST API.

### Core Principles

- **üîí Containerization First**: All workflows run in Docker containers for environment parity
- **üìä Reproducibility**: Ensure all experiments and workflows can be reproduced across different environments
- **üì¶ Versioning**: Track data, models, and experiments using DVC and MLflow
- **ü§ñ Automation**: Implement CI/CD pipelines for testing, linting, and deployment
- **üìà Monitoring**: Track model performance and detect data drift in production

### Success Metrics

- **Model Performance**: F1 Score, Precision, and Recall for accident severity classification
- **Baseline Model**: XGBoost with optimized parameters as initial benchmark
- **Minimum Performance Threshold**: To be defined based on baseline results

## üê≥ Docker & Containerization

> **‚≠ê This project is designed to run in Docker containers. Docker ensures environment parity across development, training, and production.**

The project uses a **multi-stage Dockerfile** with three stages:
- **`dev`**: Development environment with all tools and dependencies
- **`train`**: Training pipeline container for model training
- **`prod`**: Production container for inference (commented out, ready for FastAPI)

### Quick Start with Docker

**Option 1: Docker Compose (Recommended)**

```bash
# Build all services
docker compose build

# Start development shell
docker compose up dev

# Run training pipeline
docker compose up train

# Pull data/models via DVC (if configured)
docker compose --profile dvc up dvc-pull
# Or using Makefile:
make docker-dvc-pull
```

**Option 2: Makefile Commands**

```bash
# Build images
make docker-build-dev      # Development image
make docker-build-train     # Training image

# Run containers
make docker-run-dev                    # Interactive dev shell
make docker-run-train                  # Run training pipeline
make docker-run-dev-exec CMD="..."    # Run one-off command
make docker-dvc-pull                   # Pull data/models from DVC remote
```

**Option 3: Direct Docker Commands**

```bash
# Development shell
docker run -it --rm -v $(PWD):/app mlops-accidents:dev

# Training pipeline
docker run --rm -v $(PWD):/app mlops-accidents:train
```

### Docker Compose Services

The `docker-compose.yml` defines three services:

| Service | Purpose | Usage |
|---------|---------|-------|
| **`dev`** | Development environment | Interactive shell for development and testing |
| **`train`** | Training pipeline | Runs complete ML training workflow |
| **`dvc-pull`** | Data sync | Pulls data from DVC remote (optional profile) |

### Running the Pipeline in Docker

```bash
# Pull latest data/models from DVC (if configured)
make docker-dvc-pull
# Or: docker compose --profile dvc up dvc-pull

# Complete workflow in Docker
docker compose up train

# Or step-by-step in dev container
docker compose up dev
# Inside container:
make run-import      # Step 1: Download data
make run-preprocess  # Step 2: Clean & merge
make run-features    # Step 3: Feature engineering
make run-train       # Step 4: Train models
make run-predict     # Step 5: Make predictions
```

### Volume Mounts

- **`.:/app`**: Project code mounted for dev/train (read-write)
- **`./models:/app/models`**: Model artifacts (read-write for train, read-only for prod)
- **`./data:/app/data`**: Data directory (read-write for train, read-only for prod)
- **`~/.dvc:/home/mlops/.dvc`**: DVC configuration (read-only for dev/train)

### Production Deployment (Template)

The production stage is commented out in the Dockerfile. To enable:

1. **Uncomment prod stage** in `Dockerfile`
2. **Uncomment prod service** in `docker-compose.yml` (if using compose)
3. **Build and run**:
   ```bash
   make docker-build-prod
   docker run --rm -v $(PWD)/models:/app/models:ro -v $(PWD)/data:/app/data:ro mlops-accidents:prod
   ```

> **Note**: The prod stage will be updated to run FastAPI when the API is implemented (Phase 1).

## üîÑ Pipeline Overview

The ML pipeline follows a simple 5-step workflow from raw data to predictions:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  1. Data Import ‚îÇ  Downloads 4 CSV files from AWS S3
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 2. Preprocessing‚îÇ  Cleans & merges data ‚Üí interim dataset
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 3. Feature Eng. ‚îÇ  Creates ML-ready features (temporal, cyclic, interactions)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  4. Training    ‚îÇ  Trains multiple models (XGBoost, RF, LR, LightGBM) with SMOTE
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  5. Prediction  ‚îÇ  Makes predictions on new data
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Key Files & Their Roles

| Step | File | What It Does | Output |
|------|------|--------------|--------|
| **1. Import** | `src/data/import_raw_data.py` | Downloads raw CSV files from S3 | `data/raw/*.csv` |
| **2. Preprocess** | `src/data/make_dataset.py` | Merges 4 datasets, cleans data, creates target variable | `data/preprocessed/interim_dataset.csv` |
| **3. Features** | `src/features/build_features.py` | Feature engineering: temporal, cyclic encoding, interactions | `data/preprocessed/features.csv` + `models/label_encoders.joblib` |
| **4. Train** | `src/models/train_multi_model.py` | Trains multiple models, compares performance, saves models + metadata | `models/{model_type}_model.joblib` + `data/metrics/model_comparison.csv` |
| **5. Predict** | `src/models/predict_model.py` | Loads model, preprocesses input, makes predictions | Prediction results |

### Supporting Utilities

- **`src/features/preprocess.py`**: Reusable preprocessing functions for inference (ensures training/inference consistency)

### Quick Command Reference

```bash
# Run complete pipeline
make workflow-all

# Or run steps individually
make run-import      # Step 1: Download data
make run-preprocess  # Step 2: Clean & merge
make run-features    # Step 3: Feature engineering
make run-train       # Step 4: Train model
make run-predict     # Step 5: Make predictions
```

## üìä Current State

**Phase**: Phase 1 - Foundations (Deadline: December 19th, 2024)

### ‚úÖ Completed

**Project Foundation:**
- ‚úÖ Project structure based on cookiecutter-data-science template
- ‚úÖ Reproducible Python environment setup with `pyproject.toml` and UV
- ‚úÖ Development automation with Makefile
- ‚úÖ Python version management (`.python-version`, `.tool-versions`)
- ‚úÖ Initial data exploration notebook

**Containerization (Engineer C):**
- ‚úÖ **Multi-stage Dockerfile**: Dev, train, and prod stages implemented
- ‚úÖ **Docker Compose**: Services for dev, train, and dvc-pull configured
- ‚úÖ **Volume mounts**: Proper data and model persistence
- üöß **Production stage**: Template ready, awaiting FastAPI implementation

**ML Modeling & Tracking (Engineer B):**
- ‚úÖ **Feature Engineering Pipeline**: Complete feature engineering module (`build_features.py`) with temporal features, cyclic encoding, categorical transformations, and interactions
- ‚úÖ **Preprocessing Utilities**: Reusable preprocessing functions (`preprocess.py`) ensuring training/inference consistency
- ‚úÖ **Model Training**: Multi-model training framework (`train_multi_model.py`) with XGBoost, Random Forest, Logistic Regression, LightGBM + SMOTE
- ‚úÖ **Model Prediction**: Inference script (`predict_model.py`) with feature preprocessing and model artifact loading
- ‚úÖ **Model Artifacts**: Model and metadata saving (per-model files: `{model_type}_model.joblib`, `label_encoders.joblib`, `{model_type}_model_metadata.joblib`)
- ‚úÖ **Metrics Saving**: Evaluation metrics saved to files (accuracy, precision, recall, F1-score)
- ‚úÖ **ML-0**: Baseline Model Definition (XGBoost baseline implemented)
- ‚úÖ **ML-1**: Config-Driven Training (`model_config.yaml` created, all parameters moved from hardcoded values)
- ‚úÖ **ML-2**: MLflow Integration (experiment tracking and model registry with versioning and staging implemented)

### üöß In Progress

**ML Modeling & Tracking (Engineer B):**
- üöß **ML-3**: Model Evaluation (metrics saved to files, but not to DVC metrics format; confusion matrix pending)
- üöß **TEST-1**: Unit Test Implementation (test suite for models not yet created)

**Data & Pipeline Infrastructure (Engineer A):**
- üöß **DVC + Dagshub integration**: Data versioning setup in progress
- üöß **Data validation**: Pandera or manual schema validation pending

**API & CI/CD (Engineer C):**
- üöß **FastAPI service**: Basic inference API pending
- üöß **CI/CD pipeline**: GitHub Actions workflows pending

### üìù Planned

See [Roadmap](#-roadmap) section for detailed phase breakdown.

## üöÄ Getting Started

### Prerequisites

- **Docker & Docker Compose** - **Required** for containerized workflow
- **Python 3.8+** (3.11 recommended) - For local development (optional)
- **UV** - Fast Python package installer ([Install](https://github.com/astral-sh/uv): `curl -LsSf https://astral.sh/uv/install.sh | sh`)
- **DVC** - Installed automatically with dependencies
- **Git** & **Make** - Usually pre-installed
- **Dagshub account** - [Sign up](https://dagshub.com) for data versioning (optional)

> **‚ö†Ô∏è Windows Users**: This project uses Makefiles and shell scripts that require a Unix-like environment. On Windows, please use one of the following:
> - **Git Bash** (recommended) - Comes with Git for Windows
> - **WSL (Windows Subsystem for Linux)** - Full Linux environment
> - **MSYS2/MinGW** - Unix-like environment for Windows
>
> The Makefile and shell commands will not work in native Windows PowerShell or CMD.

### Quick Start (Docker - Recommended)

1. **Clone the repository**
   ```bash
   git clone https://github.com/chrmei/MLOps_accidents.git
   cd MLOps_accidents
   ```

2. **Build Docker images**
   ```bash
   docker compose build
   # Or using Makefile:
   make docker-build-dev
   make docker-build-train
   ```

3. **Run the complete pipeline**
   ```bash
   # Option 1: Using Docker Compose
   docker compose up train
   
   # Option 2: Using Makefile
   make docker-run-train
   ```

4. **Start development shell**
   ```bash
   docker compose up dev
   # Or: make docker-run-dev
   ```

### Local Development Setup (Optional)

If you prefer to run locally without Docker:

1. **Clone and setup Python**
   ```bash
   git clone https://github.com/chrmei/MLOps_accidents.git
   cd MLOps_accidents
   # Using pyenv: pyenv install 3.11.0 && pyenv local 3.11.0
   # Using asdf: asdf install python 3.11.0 && asdf reshim python
   ```

2. **Install dependencies**
   ```bash
   make install-dev  # Installs project + dev dependencies (pytest, black, isort, mypy, etc.)
   # Alternative: uv pip install -e ".[dev]"
   ```

3. **Verify installation**
   ```bash
   python --version && make help
   ```

4. **Set up DVC and Dagshub (Optional but recommended)**
   
   DVC is used for data versioning. To set it up:
   
   ```bash
   # Step 1: Create .env file from template (if .env.example exists)
   cp .env.example .env
   
   # Step 2: MANUALLY EDIT .env file with your Dagshub credentials:
   #   - DAGSHUB_USERNAME: Your Dagshub username
   #   - DAGSHUB_TOKEN: Get from https://dagshub.com/user/settings/tokens
   #   - DAGSHUB_REPO: Your repository (e.g., chrmei/MLOps_accidents)
   # 
   # IMPORTANT: You must manually edit .env before running the next commands!
   
   # Step 3: Initialize DVC and configure remote using Makefile
   make dvc-init
   make dvc-setup-remote
   ```
   
   **Important Notes**:
   - The `.env` file must be **manually edited** with your credentials before running `make dvc-setup-remote`
   - The `.env` file is gitignored and will never be committed
   - Each team member should create their own `.env` file with their personal Dagshub credentials

### Run the Pipeline

**In Docker (Recommended):**
```bash
# Complete workflow
docker compose up train

# Or step-by-step in dev container
docker compose up dev
# Inside container:
make workflow-all
```

**Locally:**
```bash
# 1. Import raw data (downloads 4 CSV files from AWS S3)
make run-import

# 2. Preprocess data (creates train/test splits in data/preprocessed/)
make run-preprocess

# 3. Train models (saves to models/{model_type}_model.joblib)
make run-train

# 4. Make predictions
make run-predict                    # Interactive mode
make run-predict-file FILE=path     # From JSON file
```

## üìÅ Project Structure

```
MLOps_accidents/
‚îú‚îÄ‚îÄ .github/workflows/         # CI/CD pipelines (to be implemented)
‚îÇ   ‚îú‚îÄ‚îÄ lint.yaml
‚îÇ   ‚îî‚îÄ‚îÄ test.yaml
‚îú‚îÄ‚îÄ data/                      # Data directory (created at runtime)
‚îÇ   ‚îú‚îÄ‚îÄ external/              # Data from third party sources
‚îÇ   ‚îú‚îÄ‚îÄ interim/               # Intermediate data that has been transformed
‚îÇ   ‚îú‚îÄ‚îÄ processed/             # The final, canonical data sets for modeling
‚îÇ   ‚îî‚îÄ‚îÄ raw/                   # The original, immutable data dump
‚îú‚îÄ‚îÄ doc/                       # Project documentation
‚îÇ   ‚îú‚îÄ‚îÄ README_INITIAL.md      # Initial project documentation
‚îÇ   ‚îú‚îÄ‚îÄ Plan_Phase_01.md       # Detailed Phase 1 execution plan
‚îÇ   ‚îî‚îÄ‚îÄ Roadmap.md             # Project roadmap and milestones
‚îú‚îÄ‚îÄ logs/                      # Logs from training and predicting
‚îú‚îÄ‚îÄ models/                    # Trained and serialized models
‚îú‚îÄ‚îÄ notebooks/                 # Jupyter notebooks
‚îÇ   ‚îî‚îÄ‚îÄ 1.0-ldj-initial-data-exploration.ipynb
‚îú‚îÄ‚îÄ references/                # Data dictionaries, manuals, and explanatory materials
‚îú‚îÄ‚îÄ reports/                   # Generated analysis as HTML, PDF, LaTeX, etc.
‚îÇ   ‚îî‚îÄ‚îÄ figures/               # Generated graphics and figures
‚îú‚îÄ‚îÄ src/                       # Source code for use in this project
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ config/                # Configuration files (YAML configs)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ model_config.yaml
‚îÇ   ‚îú‚îÄ‚îÄ data/                  # Scripts to download or generate data
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ check_structure.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ import_raw_data.py # Downloads data from S3
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ make_dataset.py    # Preprocesses raw data
‚îÇ   ‚îú‚îÄ‚îÄ features/              # Scripts to turn raw data into features
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ build_features.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ preprocess.py      # Reusable preprocessing for inference
‚îÇ   ‚îú‚îÄ‚îÄ models/                # Scripts to train models and make predictions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ predict_model.py   # Model inference script
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_multi_model.py # Multi-model training framework
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_features.json # Example features for testing
‚îÇ   ‚îî‚îÄ‚îÄ visualization/         # Scripts to create visualizations
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ visualize.py
‚îú‚îÄ‚îÄ scripts/                   # Utility scripts
‚îÇ   ‚îú‚îÄ‚îÄ manage_model_registry.py # MLflow model registry management
‚îÇ   ‚îî‚îÄ‚îÄ setup_dvc_remote.sh
‚îú‚îÄ‚îÄ tests/                     # Pytest suite (to be implemented)
‚îÇ   ‚îú‚îÄ‚îÄ test_data.py
‚îÇ   ‚îú‚îÄ‚îÄ test_models.py
‚îÇ   ‚îî‚îÄ‚îÄ test_api.py
‚îú‚îÄ‚îÄ Dockerfile                 # Multi-stage Dockerfile
‚îú‚îÄ‚îÄ docker-compose.yml         # Docker Compose configuration
‚îú‚îÄ‚îÄ dvc.yaml                   # DVC pipeline definition
‚îú‚îÄ‚îÄ LICENSE                    # MIT License
‚îú‚îÄ‚îÄ Makefile                   # Development commands and automation
‚îú‚îÄ‚îÄ pyproject.toml             # Python project configuration and dependencies
‚îú‚îÄ‚îÄ requirements.txt           # Python dependencies (legacy, use pyproject.toml)
‚îú‚îÄ‚îÄ setup.py                   # Package setup configuration (legacy)
‚îî‚îÄ‚îÄ README.md                  # This file
```

## üõ†Ô∏è Technology Stack

| Component | Technology | Role |
| :--- | :--- | :--- |
| **Containerization** | **Docker** | Multi-stage containers for dev, train, and prod |
| **Data Versioning** | **DVC + Dagshub** | Versioning raw data and artifacts without AWS |
| **Experiment Tracking** | **MLflow (Dagshub)** | Tracking metrics, parameters, and models |
| **Model Serving** | **FastAPI** | REST API for real-time accident prediction (planned) |
| **CI/CD** | **GitHub Actions** | Automated testing, linting, and image building (planned) |
| **Machine Learning** | **scikit-learn, XGBoost, LightGBM** | Model training and evaluation |
| **Data Processing** | **pandas, numpy** | Data manipulation and preprocessing |
| **Testing** | **pytest** | Unit and integration testing |
| **Code Quality** | **black, flake8, isort** | Code formatting and linting |
| **Dependency Management** | **UV** | Fast Python package installer and resolver |
| **Build System** | **setuptools** | Package building and distribution |

## üõ†Ô∏è Development Commands

Run `make help` to see all commands. Key commands:

**Setup & Dependencies**
- `make install-dev` - Install with dev dependencies
- `make setup-venv` - Create venv and install dependencies
- `make clean` - Remove build artifacts

**Code Quality**
- `make format` - Format with black & isort
- `make lint` - Lint with flake8
- `make type-check` - Type checking with mypy

**Testing**
- `make test` - Run pytest
- `make test-cov` - Run with coverage report

**Data Pipeline**
- `make run-import` - Import raw data
- `make run-preprocess` - Preprocess data
- `make run-features` - Build features
- `make run-train` - Train models
- `make run-predict` - Interactive predictions
- `make run-predict-file FILE=path` - Predictions from JSON

**Docker**
- `make docker-build-dev` - Build development image
- `make docker-build-train` - Build training image
- `make docker-run-dev` - Run dev container (interactive)
- `make docker-run-train` - Run training pipeline
- `make docker-run-dev-exec CMD="..."` - Run one-off command
- `make docker-dvc-pull` - Pull data/models from DVC remote via Docker Compose

**DVC (Data Version Control)**
- `make dvc-init` - Initialize DVC
- `make dvc-setup-remote` - Configure Dagshub remote (**requires manually edited .env**)
- `make dvc-status` / `dvc-push` / `dvc-pull` / `dvc-repro`

> **Important**: For `make dvc-setup-remote`, you must first manually edit `.env` with your Dagshub credentials (copy from `.env.example`).

## üîÑ Workflow

### Detailed Pipeline Steps

#### Step 1: Data Import (`src/data/import_raw_data.py`)
- **Purpose**: Download raw data from AWS S3
- **Input**: None (downloads from S3)
- **Output**: 4 CSV files in `data/raw/`
  - `caracteristiques-2021.csv` (accident characteristics)
  - `lieux-2021.csv` (location data)
  - `usagers-2021.csv` (victim/user data)
  - `vehicules-2021.csv` (vehicle data)

#### Step 2: Data Preprocessing (`src/data/make_dataset.py`)
- **Purpose**: Clean and merge raw data into a single dataset
- **Input**: 4 raw CSV files
- **Process**:
  - Merges all datasets on accident ID (`Num_Acc`)
  - Cleans data (handles missing values, converts types)
  - Creates aggregations (`nb_victim`, `nb_vehicules`)
  - Transforms target variable to binary classification
- **Output**: `data/preprocessed/interim_dataset.csv`

#### Step 3: Feature Engineering (`src/features/build_features.py`)
- **Purpose**: Transform interim data into ML-ready features
- **Input**: `interim_dataset.csv`
- **Process**:
  - **Temporal features**: Creates datetime, extracts hour/month/day, cyclic encoding
  - **Age features**: Calculates victim age, creates age bins
  - **Categorical transformations**: Groups vehicle types, atmospheric conditions
  - **Interactions**: Creates feature interactions (e.g., `victims_per_vehicle`)
  - **Encoding**: Label encodes categorical features
- **Output**: 
  - `data/preprocessed/features.csv` (feature-engineered dataset)
  - `models/label_encoders.joblib` (saved encoders for inference)

#### Step 4: Model Training (`src/models/train_multi_model.py`)
- **Purpose**: Train multiple models (XGBoost, Random Forest, Logistic Regression, LightGBM) with SMOTE for imbalanced data
- **Input**: `features.csv`
- **Process**:
  - Splits data into train/test sets (same split for all models for fair comparison)
  - Applies SMOTE (oversampling) to handle class imbalance
  - Trains multiple model types (with or without grid search)
  - Evaluates each model's performance
  - Generates model comparison report
- **Output**:
  - `models/{model_type}_model.joblib` (trained model pipelines, e.g., `xgboost_model.joblib`)
  - `models/{model_type}_model_metadata.joblib` (feature names, config per model)
  - `data/metrics/{model_type}_metrics.json` (evaluation metrics per model)
  - `data/metrics/model_comparison.csv` (comparison report ranking models by F1 score)

#### Step 5: Prediction (`src/models/predict_model.py`)
- **Purpose**: Make predictions on new data
- **Input**: JSON file with input features
- **Process**:
  - Loads trained model and artifacts (encoders, metadata)
  - Preprocesses input using same pipeline as training (`src/features/preprocess.py`)
  - Aligns features with model expectations
  - Makes prediction
- **Output**: Prediction result (0 = Non-Priority, 1 = Priority)

### Complete Workflow Command

```bash
# Run entire pipeline in one command
make workflow-all

# Or run steps individually for more control
make run-import      # Step 1: Download raw data
make run-preprocess  # Step 2: Create interim dataset
make run-features    # Step 3: Build features
make run-train       # Step 4: Train model
make run-predict     # Step 5: Make predictions
```

### Reproducing the Workflow using DVC

Run `make dvc-repro` to reproduce the workflow using default configurations. This will:

1. Pull the latest version of raw data from the remote storage using `dvc pull`

2. Complete the workflow from [Step 2](#step-2-data-preprocessing-srcdatamake_datasetpy) on

Note that DVC needs to be set up first using `make dvc-setup-remote`.
 
The default configurations are defined [here](src/config/model_config.yaml) and the prediction step is done on test features defined [here](src/models/test_features.json), which finally should output

```
Prediction: 1
Interpretation: Priority
```

**Target (Post Phase 1)**: DVC Pipeline ‚Üí MLflow Tracking ‚Üí FastAPI Serving ‚Üí CI/CD

## üì¶ MLflow Model Registry

The project uses MLflow Model Registry for model versioning and staging. Models are automatically registered during training, and you can manage their lifecycle through staging transitions.

### Configuration

Model registry settings are configured in `src/config/model_config.yaml`:

```yaml
mlflow:
  enabled: true
  tracking_uri: ""  # Set via MLFLOW_TRACKING_URI or DAGSHUB_REPO env vars
  experiment_name: "accident_prediction"
  log_model: true
  log_artifacts: true
  model_registry:
    registered_model_name: "Accident_Prediction"  # Base name - model type will be appended automatically
    default_stage: "None"  # Options: None, Staging, Production, Archived
    auto_transition_to_staging: false  # Auto-promote to Staging after registration
    production_stage: "Production"
```

### Quick Start: Model Staging Workflow

Follow these steps to train, stage, and deploy models using the MLflow Model Registry:

#### Step 1: Set Up Environment

Configure your MLflow tracking URI (choose one method):

```bash
# Option 1: Set environment variable directly
export MLFLOW_TRACKING_URI="https://dagshub.com/yourusername/yourrepo.mlflow"

# Option 2: Use DAGSHUB_REPO (auto-constructs URI)
export DAGSHUB_REPO="yourusername/yourrepo"
```

Or add to your `.env` file for persistence.

#### Step 2: Train and Register Models

Train models - they will be automatically registered:

```bash
# Train multiple models (default - creates versions for each model type)
make run-train

# Or with grid search for hyperparameter tuning
make run-train-grid

# Or train specific models only
python src/models/train_multi_model.py --models xgboost random_forest
```

**What happens:**
- Multiple models are trained (XGBoost, Random Forest, Logistic Regression, LightGBM by default)
- Each model is saved locally to `models/{model_type}_model.joblib`
- Each model is automatically registered to MLflow Model Registry with name `Accident_Prediction_{ModelType}`
- New versions are created for each model (starts in "None" stage)
- Metrics, parameters, and artifacts are logged to MLflow for each model
- Model comparison report is generated at `data/metrics/model_comparison.csv`

#### Step 3: Check Registered Models

View what's in your registry:

```bash
# See all registered models
python scripts/manage_model_registry.py list-models

# See all versions of your specific model
python scripts/manage_model_registry.py list-versions \
  --model-name Accident_Prediction_XGBoost
```

#### Step 4: Move Model to Staging

After validating the model locally, promote it to Staging for testing:

```bash
# Move version 1 to Staging
python scripts/manage_model_registry.py transition \
  --model-name Accident_Prediction_XGBoost \
  --version 1 \
  --stage Staging
```

#### Step 5: Test Model from Staging

Test the Staging model to ensure it works correctly:

```bash
# Make predictions using Staging model
python src/models/predict_model.py src/models/test_features.json \
  --model-name Accident_Prediction_XGBoost \
  --stage Staging
```

#### Step 6: Promote to Production

Once testing passes, promote to Production:

```bash
# Option A: Promote latest version (recommended)
python scripts/manage_model_registry.py promote \
  --model-name Accident_Prediction_XGBoost \
  --stage Production

# Option B: Promote specific version
python scripts/manage_model_registry.py transition \
  --model-name Accident_Prediction_XGBoost \
  --version 1 \
  --stage Production
```

#### Step 7: Use Production Model

In production environments, always load from the Production stage:

```bash
# Load from Production stage (recommended)
python src/models/predict_model.py src/models/test_features.json \
  --model-name Accident_Prediction_XGBoost \
  --stage Production
```

#### Step 8: Archive Old Models

When a model is deprecated, archive it (don't delete - maintains history):

```bash
# Archive old version
python scripts/manage_model_registry.py archive \
  --model-name Accident_Prediction_XGBoost \
  --version 1
```

### Complete Example Workflow

Here's a complete example from training to production:

```bash
# 1. Set up environment
export DAGSHUB_REPO="yourusername/yourrepo"

# 2. Train a new model
make run-train
# Output: Model registered as 'Accident_Prediction_XGBoost' version 1

# 3. Check what was registered
python scripts/manage_model_registry.py list-versions \
  --model-name Accident_Prediction_XGBoost
# You'll see version 1 in "None" stage

# 4. Move to Staging for testing
python scripts/manage_model_registry.py transition \
  --model-name Accident_Prediction_XGBoost \
  --version 1 \
  --stage Staging

# 5. Test the Staging model
python src/models/predict_model.py src/models/test_features.json \
  --model-name Accident_Prediction_XGBoost \
  --stage Staging

# 6. If tests pass, promote to Production
python scripts/manage_model_registry.py transition \
  --model-name Accident_Prediction_XGBoost \
  --version 1 \
  --stage Production

# 7. Use Production model
python src/models/predict_model.py src/models/test_features.json \
  --model-name Accident_Prediction_XGBoost \
  --stage Production

# 8. Later, train a better model (creates version 2)
make run-train

# 9. After validating version 2, promote it
python scripts/manage_model_registry.py promote \
  --model-name Accident_Prediction_XGBoost \
  --stage Production
# This moves version 2 to Production

# 10. Archive old version 1
python scripts/manage_model_registry.py archive \
  --model-name Accident_Prediction_XGBoost \
  --version 1
```

### Model Staging Lifecycle

Models progress through the following stages:

1. **None** (default) - Newly registered models start here
2. **Staging** - Models under evaluation/testing
3. **Production** - Models deployed and serving predictions
4. **Archived** - Deprecated models

### Managing Models

Use the `scripts/manage_model_registry.py` script to manage models:

#### List Registered Models

```bash
# List all registered models
python scripts/manage_model_registry.py list-models

# List all versions of a specific model
python scripts/manage_model_registry.py list-versions --model-name Accident_Prediction_XGBoost
```

#### Transition Models Between Stages

```bash
# Transition a specific version to Production
python scripts/manage_model_registry.py transition \
  --model-name Accident_Prediction_XGBoost \
  --version 1 \
  --stage Production

# Promote latest version to Production
python scripts/manage_model_registry.py promote \
  --model-name Accident_Prediction_XGBoost \
  --stage Production

# Archive an old version
python scripts/manage_model_registry.py archive \
  --model-name Accident_Prediction_XGBoost \
  --version 1
```

#### Get Model Information

```bash
# Get model info by stage
python scripts/manage_model_registry.py get-model \
  --model-name Accident_Prediction_XGBoost \
  --stage Production
```

### Loading Models from Registry

The prediction script (`src/models/predict_model.py`) supports loading models from the MLflow registry:

**Best Practice: Use MLflow Model Registry for Production Inference**

```bash
# Automatically use best Production model across all model types (recommended)
python src/models/predict_model.py src/models/test_features.json \
  --use-best-model

# Or load from Production stage for specific model type
python src/models/predict_model.py src/models/test_features.json \
  --use-mlflow-production

# Or explicitly specify model name and stage
python src/models/predict_model.py src/models/test_features.json \
  --model-name Accident_Prediction_XGBoost \
  --stage Production

# Load specific version
python src/models/predict_model.py src/models/test_features.json \
  --model-name Accident_Prediction_XGBoost \
  --version 6

# Load from local filesystem (for development/testing only)
python src/models/predict_model.py src/models/test_features.json \
  --model-path models/xgboost_model.joblib

# Use environment variables
export USE_BEST_MODEL=true  # Auto-select best model
# or
export USE_MLFLOW_PRODUCTION=true  # Use default model type (XGBoost)
python src/models/predict_model.py src/models/test_features.json
```

**Architecture:**
- **MLflow Model Registry**: Used for production model serving (default with `--use-mlflow-production`)
- **Local filesystem (DVC)**: Used for development/testing and pipeline reproducibility
- Models are tracked in DVC for reproducible training pipelines, but production inference loads from MLflow

### Automatic Staging Transitions

You can enable automatic transition to Staging after model registration by setting `auto_transition_to_staging: true` in the config. This is useful for automated workflows where new models should be immediately available for testing.

### Model Storage Architecture (Best Practices)

**MLflow Model Registry** (for models):
- ‚úÖ Production model serving and deployment
- ‚úÖ Model versioning and lifecycle management (Staging ‚Üí Production ‚Üí Archived)
- ‚úÖ Model metadata, metrics, and parameters tracking
- ‚úÖ Experiment tracking and model comparison

**DVC** (for data):
- ‚úÖ Data pipeline reproducibility (raw ‚Üí preprocessed ‚Üí features)
- ‚úÖ Data versioning and tracking
- ‚úÖ Label encoders and preprocessing artifacts
- ‚úÖ Metrics files for pipeline tracking
- ‚ö†Ô∏è Model files tracked only for pipeline reproducibility (not for production use)

**Key Points:**
- Production inference should load from MLflow Model Registry (Production stage)
- **Multi-model setup**: Use `--use-best-model` to automatically select the best performing Production model across all model types (XGBoost, RandomForest, etc.)
- Each model type is registered separately: `Accident_Prediction_XGBoost`, `Accident_Prediction_Random_Forest`, etc.
- Local model files in DVC are for development/testing and pipeline reproducibility only
- Use `--use-mlflow-production` flag or `USE_MLFLOW_PRODUCTION=true` for production inference (defaults to XGBoost)
- Use `--use-best-model` flag or `USE_BEST_MODEL=true` to auto-select best model
- `make dvc-pull` pulls data and training artifacts for local development, not production models

### Best Practices

1. **Version Control**: Each training run creates a new model version automatically
2. **Staging Workflow**: 
   - New models ‚Üí None stage
   - After validation ‚Üí Staging stage
   - After approval ‚Üí Production stage
3. **Production Models**: Always load from Production stage in production environments
4. **Archiving**: Archive old models instead of deleting them to maintain history

### Environment Setup

Set up MLflow tracking URI via environment variables:

```bash
# Option 1: Direct MLflow URI
export MLFLOW_TRACKING_URI="https://dagshub.com/username/repo.mlflow"

# Option 2: Use DAGSHUB_REPO (auto-constructs URI)
export DAGSHUB_REPO="username/repo"
```

The tracking URI can also be set in `model_config.yaml`, but environment variables take precedence.

## üîÑ Multi-Model Training Framework

**The multi-model training framework is now the default training method.** The project includes a standardized framework for training and comparing multiple ML models. This framework enables easy experimentation with different algorithms (XGBoost, Random Forest, Logistic Regression, LightGBM) and automatic comparison of their performance.

### Features

- **Standardized Training**: All models use the same train/test split for fair comparison
- **MLflow Integration**: Models are automatically logged with type-specific tags for easy filtering
- **Model Registry**: Each model type gets its own registered model name (format: `Accident_Prediction_{ModelType}`)
- **Automatic Comparison**: Generates comparison reports ranking models by performance metrics
- **Extensible**: Easy to add new model types by creating a trainer class
- **Default Training**: Used by default in `make run-train` and DVC pipeline

### Quick Start

```bash
# Train all enabled models (default)
make run-train

# Train with grid search for hyperparameter tuning
make run-train-grid

# Train specific models only
python src/models/train_multi_model.py --models xgboost random_forest

# Train single model (legacy mode)
make run-train-single
```

### Output Files

After training, you'll find:
- **Models**: `models/{model_type}_model.joblib` (e.g., `models/xgboost_model.joblib`)
- **Metadata**: `models/{model_type}_model_metadata.joblib`
- **Metrics**: `data/metrics/{model_type}_metrics.json`
- **Comparison**: `data/metrics/model_comparison.csv` (ranks models by F1 score)

### Model Names in Registry

Models are registered with the format `Accident_Prediction_{ModelType}`:
- `Accident_Prediction_XGBoost`
- `Accident_Prediction_Random_Forest`
- `Accident_Prediction_Logistic_Regression`
- `Accident_Prediction_Lightgbm`

This naming convention groups all models under the project prefix for easy identification in MLflow.

For detailed documentation on the multi-model training framework, including how to add new models, see the [Multi-Model Training README](src/models/README_MULTI_MODEL.md).

### Quick Reference: Common Commands

```bash
# LIST MODELS
python scripts/manage_model_registry.py list-models
python scripts/manage_model_registry.py list-versions --model-name Accident_Prediction_XGBoost

# TRANSITION STAGES
python scripts/manage_model_registry.py transition --model-name Accident_Prediction_XGBoost --version 1 --stage Staging
python scripts/manage_model_registry.py transition --model-name Accident_Prediction_XGBoost --version 1 --stage Production

# PROMOTE LATEST VERSION
python scripts/manage_model_registry.py promote --model-name Accident_Prediction_XGBoost --stage Production

# ARCHIVE OLD MODEL
python scripts/manage_model_registry.py archive --model-name Accident_Prediction_XGBoost --version 1

# GET MODEL INFO
python scripts/manage_model_registry.py get-model --model-name Accident_Prediction_XGBoost --stage Production

# USE IN PREDICTIONS
python src/models/predict_model.py file.json --model-name Accident_Prediction_XGBoost --stage Production
python src/models/predict_model.py file.json --model-name Accident_Prediction_XGBoost --version 1
python src/models/predict_model.py file.json --model-path models/trained_model.joblib  # Local filesystem
```

## üë• Team Structure

The project is designed for a 3-person team with clear separation of concerns:

### **Engineer A: Data & Pipeline Infrastructure**
- **Focus**: Getting data from raw to "model-ready"
- **Deliverables**: DVC pipelines, data validation (Pandera or manual), Dagshub integration
- **Primary Files**: `src/data/`, `dvc.yaml`, `params.yaml`
- **Status**: 
  - üöß DVC + Dagshub integration in progress
  - üöß Data validation pending

### **Engineer B: ML Modeling & Tracking**
- **Focus**: ML model development and experiment logging
- **Deliverables**: Training scripts, MLflow tracking, model registry management
- **Primary Files**: `src/models/`, `src/features/`, `src/config/model_config.yaml`
- **Status**: 
  - ‚úÖ Feature engineering pipeline complete
  - ‚úÖ Model training and prediction scripts functional
  - ‚úÖ Config-driven training (ML-1) - Complete
  - ‚úÖ MLflow integration (ML-2) - Model registry with versioning and staging implemented
  - üöß DVC metrics format (ML-3) - Partial (metrics saved, DVC format pending)
  - üöß Unit tests (TEST-1) - Pending

### **Engineer C: API, Docker & CI/CD**
- **Focus**: Containerization, API development, and automation
- **Deliverables**: FastAPI application, Dockerfiles, GitHub Actions pipelines
- **Primary Files**: `src/api/`, `Dockerfile`, `.github/workflows/`
- **Status**:
  - ‚úÖ Multi-stage Dockerfile implemented (dev, train, prod template)
  - ‚úÖ Docker Compose configuration complete
  - üöß FastAPI service pending
  - üöß CI/CD pipeline pending

## üó∫Ô∏è Roadmap

### Phase 1: Foundations (Deadline: December 19th, 2024)

**Containerization (Engineer C) Progress:**
- ‚úÖ Multi-stage Dockerfile (dev, train, prod template)
- ‚úÖ Docker Compose services configured
- ‚úÖ Volume mounts and environment setup
- üöß Production stage ready for FastAPI integration

**ML Modeling & Tracking (Engineer B) Progress:**
- ‚úÖ Feature engineering pipeline (`build_features.py`, `preprocess.py`)
- ‚úÖ Multi-model training framework (`train_multi_model.py`) with XGBoost, Random Forest, Logistic Regression, LightGBM + SMOTE
- ‚úÖ Model prediction script (`predict_model.py`) with preprocessing
- ‚úÖ Model artifacts and metadata saving (per-model files)
- ‚úÖ **ML-0**: XGBoost baseline model implemented
- ‚úÖ **ML-1**: Config-driven training (`model_config.yaml` created, all parameters moved)
- ‚úÖ **ML-2**: MLflow integration for experiment tracking and model registry
- ‚úÖ **Multi-Model Framework**: Standardized framework for training and comparing multiple models
- üöß **ML-3**: DVC metrics format and confusion matrix (partial - metrics saved, DVC format pending)
- üöß **TEST-1**: Unit tests for model training and prediction

**Overall Phase 1 Status:**
- ‚úÖ Define project objectives and key metrics
- ‚úÖ Set up reproducible development environment (containerization, Docker) - **Docker implemented**
- ‚úÖ Collect and preprocess data (ML Pipeline) - **Data pipeline functional**
- üöß Build and evaluate baseline ML model, implement unit tests - **Model training functional, baseline complete, tests pending**
- üöß Implement basic inference API - **Pending**

### Phase 2: Microservices, Tracking & Versioning (Deadline: January 16th, 2026)
- ‚úÖ Set up experiment tracking with MLflow
- üöß Implement data & model versioning (MLflow, DVC)
- Decompose application into microservices and design orchestration

### Phase 3: Orchestration & Deployment (Deadline: January 29th, 2026)
- Finalize end-to-end orchestration
- Create CI Pipeline (GitHub Actions: linter and others)
- Optimize and secure the API
- Implement scalability with Docker/Kubernetes

### Phase 4: Monitoring & Maintenance (Deadline: February 6th, 2026)
- Set up performance monitoring using Prometheus/Grafana
- Implement drift detection with Evidently
- Develop automated model and component updates
- Finalize technical documentation

**Final Presentation (Defence)**: February 9th, 2026

For detailed execution plans, see:
- [Phase 1 Plan](doc/Plan_Phase_01.md)
- [Roadmap](doc/Roadmap.md)

## üìù Development Guidelines

**Code**: Run scripts from project root, use conventional commits (`feat:`, `fix:`), follow PEP 8, add type hints  
**Branches**: `feature/<ticket-id>-description`, PRs require approval + passing CI  
**Testing**: >70% coverage for `src/data/`, `src/models/`, `src/api/`; run `make test` before PRs  
**Dependencies**: Managed in `pyproject.toml` via UV (`make install-dev`); Python version in `.python-version`  
**Containerization**: All workflows should run in Docker containers for reproducibility

## ü§ù Contributing

1. Setup: `make install-dev` (or use Docker: `docker compose up dev`)
2. Branch: `git checkout -b feature/your-feature-name`
3. Develop: Make changes following guidelines
4. Quality: `make format && make lint && make type-check`
5. Test: `make test` (coverage: `make test-cov`)
6. PR: Ensure all checks pass, submit with clear description

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- Project based on the [cookiecutter data science project template](https://drivendata.github.io/cookiecutter-data-science/)
- Data sourced from French road accident databases

## üìö Additional Documentation

- [Initial README](doc/README_INITIAL.md) - Original project documentation
- [Phase 1 Plan](doc/Plan_Phase_01.md) - Detailed Phase 1 execution plan
- [Roadmap](doc/Roadmap.md) - Project roadmap and milestones

---

**Note**: This project is in active development. The structure and workflows are being refined as we progress through Phase 1. For the most up-to-date information, refer to the documentation in the `doc/` directory.
