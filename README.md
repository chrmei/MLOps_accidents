# ğŸš¦ MLOps Road Accident Prediction

A containerized MLOps project for predicting road accidents using machine learning. This project implements a complete MLOps workflow from data ingestion to model serving, with a focus on reproducibility, versioning, and best practices.

## ğŸ“‹ Table of Contents

- [Project Overview](#project-overview)
- [Current State](#current-state)
- [Project Structure](#project-structure)
- [Technology Stack](#technology-stack)
- [Getting Started](#getting-started)
- [Workflow](#workflow)
- [Team Structure](#team-structure)
- [Roadmap](#roadmap)
- [Contributing](#contributing)
- [License](#license)

## ğŸ¯ Project Overview

This project is an MLOps implementation for road accident prediction, designed as a learning and production-ready template. The system processes road accident data from multiple sources, trains machine learning models to predict accident severity, and serves predictions through a REST API.

### Objectives

- **Reproducibility**: Ensure all experiments and workflows can be reproduced across different environments
- **Versioning**: Track data, models, and experiments using DVC and MLflow
- **Containerization**: Isolate environments using Docker for consistent deployments
- **Automation**: Implement CI/CD pipelines for testing, linting, and deployment
- **Monitoring**: Track model performance and detect data drift in production

### Success Metrics

- **Model Performance**: F1 Score, Precision, and Recall for accident severity classification
- **Baseline Model**: RandomForest with default parameters as initial benchmark
- **Minimum Performance Threshold**: To be defined based on baseline results

## ğŸ“Š Current State

**Phase**: Phase 1 - Foundations (Deadline: December 19th)

### âœ… Completed

- Project structure based on cookiecutter-data-science template
- Reproducible Python environment setup with `pyproject.toml` and UV
- Development automation with Makefile
- Python version management (`.python-version`, `.tool-versions`)
- Data import pipeline (`import_raw_data.py`)
- Data preprocessing pipeline (`make_dataset.py`)
- Baseline model training (`train_model.py`)
- Model prediction script (`predict_model.py`)
- Initial data exploration notebook

### ğŸš§ In Progress

- Containerization with Docker
- DVC + Dagshub integration for data versioning
- MLflow integration for experiment tracking
- FastAPI service for model serving
- Unit testing suite
- CI/CD pipeline with GitHub Actions

### ğŸ“ Planned

See [Roadmap](#roadmap) section for detailed phase breakdown.

## ğŸ“ Project Structure

```
MLOps_accidents/
â”œâ”€â”€ .github/workflows/         # CI/CD pipelines (to be implemented)
â”‚   â”œâ”€â”€ lint.yaml
â”‚   â””â”€â”€ test.yaml
â”œâ”€â”€ data/                      # Data directory (created at runtime)
â”‚   â”œâ”€â”€ external/              # Data from third party sources
â”‚   â”œâ”€â”€ interim/               # Intermediate data that has been transformed
â”‚   â”œâ”€â”€ processed/             # The final, canonical data sets for modeling
â”‚   â””â”€â”€ raw/                   # The original, immutable data dump
â”œâ”€â”€ doc/                       # Project documentation
â”‚   â”œâ”€â”€ README_INITIAL.md      # Initial project documentation
â”‚   â”œâ”€â”€ Plan_Phase_01.md       # Detailed Phase 1 execution plan
â”‚   â””â”€â”€ Roadmap.md             # Project roadmap and milestones
â”œâ”€â”€ logs/                      # Logs from training and predicting
â”œâ”€â”€ models/                    # Trained and serialized models
â”œâ”€â”€ notebooks/                 # Jupyter notebooks
â”‚   â””â”€â”€ 1.0-ldj-initial-data-exploration.ipynb
â”œâ”€â”€ references/                # Data dictionaries, manuals, and explanatory materials
â”œâ”€â”€ reports/                   # Generated analysis as HTML, PDF, LaTeX, etc.
â”‚   â””â”€â”€ figures/               # Generated graphics and figures
â”œâ”€â”€ src/                       # Source code for use in this project
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config/                # Configuration files (YAML configs)
â”‚   â”œâ”€â”€ data/                  # Scripts to download or generate data
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ check_structure.py
â”‚   â”‚   â”œâ”€â”€ import_raw_data.py # Downloads data from S3
â”‚   â”‚   â””â”€â”€ make_dataset.py    # Preprocesses raw data
â”‚   â”œâ”€â”€ features/              # Scripts to turn raw data into features
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ build_features.py
â”‚   â”œâ”€â”€ models/                # Scripts to train models and make predictions
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ predict_model.py   # Model inference script
â”‚   â”‚   â”œâ”€â”€ train_model.py     # Model training script
â”‚   â”‚   â””â”€â”€ test_features.json # Example features for testing
â”‚   â””â”€â”€ visualization/         # Scripts to create visualizations
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ visualize.py
â”œâ”€â”€ tests/                     # Pytest suite (to be implemented)
â”‚   â”œâ”€â”€ test_data.py
â”‚   â”œâ”€â”€ test_models.py
â”‚   â””â”€â”€ test_api.py
â”œâ”€â”€ .dockerignore              # Docker ignore file (to be created)
â”œâ”€â”€ .python-version            # Python version for pyenv
â”œâ”€â”€ .tool-versions             # Python version for asdf
â”œâ”€â”€ Dockerfile                 # Multi-stage Dockerfile (to be created)
â”œâ”€â”€ docker-compose.yaml        # Local orchestration (to be created)
â”œâ”€â”€ dvc.yaml                   # DVC pipeline definition (to be created)
â”œâ”€â”€ LICENSE                    # MIT License
â”œâ”€â”€ Makefile                   # Development commands and automation
â”œâ”€â”€ pyproject.toml             # Python project configuration and dependencies
â”œâ”€â”€ requirements.txt           # Python dependencies (legacy, use pyproject.toml)
â”œâ”€â”€ setup.py                   # Package setup configuration (legacy)
â””â”€â”€ README.md                  # This file
```

## ğŸ› ï¸ Technology Stack

| Component | Technology | Role |
| :--- | :--- | :--- |
| **Data Versioning** | **DVC + Dagshub** | Versioning raw data and artifacts without AWS |
| **Experiment Tracking** | **MLflow (Dagshub)** | Tracking metrics, parameters, and models |
| **Pipeline Stages** | **Docker Containers** | Isolated environments for Data, Training, and API |
| **Model Serving** | **FastAPI** | REST API for real-time accident prediction |
| **CI/CD** | **GitHub Actions** | Automated testing, linting, and image building |
| **Machine Learning** | **scikit-learn** | Model training and evaluation |
| **Data Processing** | **pandas, numpy** | Data manipulation and preprocessing |
| **Testing** | **pytest** | Unit and integration testing |
| **Code Quality** | **black, flake8, isort** | Code formatting and linting |
| **Dependency Management** | **UV** | Fast Python package installer and resolver |
| **Build System** | **setuptools** | Package building and distribution |

## ğŸš€ Getting Started

### Prerequisites

- **Python 3.8+** (Python 3.11 recommended - specified in `.python-version` and `.tool-versions`)
- **UV** - Fast Python package installer ([Installation guide](https://github.com/astral-sh/uv))
- **Docker and Docker Compose** (for containerized workflow)
- **Git**
- **Make** (for using Makefile commands - usually pre-installed on Linux/Mac)

### Installing UV

If you don't have UV installed, you can install it with:

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

Or visit the [UV installation guide](https://github.com/astral-sh/uv) for other installation methods.

### Installation

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd MLOps_accidents
   ```

2. **Set up Python version** (optional but recommended)
   
   If using **pyenv**:
   ```bash
   pyenv install 3.11.0  # Install Python 3.11 if not already installed
   pyenv local 3.11.0    # Use the version specified in .python-version
   ```
   
   If using **asdf**:
   ```bash
   asdf install python 3.11.0  # Install Python 3.11 if not already installed
   asdf reshim python           # Reshim after installation
   ```

3. **Install dependencies using UV and Make**
   ```bash
   make install-dev
   ```
   
   This will:
   - Check if UV is installed
   - Install the project in editable mode with all dependencies
   - Include development dependencies (pytest, black, isort, mypy, etc.)
   
   **Alternative**: Install directly with UV
   ```bash
   uv pip install -e ".[dev]"
   ```

4. **Verify installation**
   ```bash
   python --version  # Should show Python 3.8 or higher
   make help         # View all available Make commands
   ```

### Initial Setup

You can use Make commands for common tasks:

1. **Import raw data**
   ```bash
   make run-import
   # or
   python src/data/import_raw_data.py
   ```
   This will download 4 datasets from AWS S3:
   - `caracteristiques-2021.csv`
   - `lieux-2021.csv`
   - `usagers-2021.csv`
   - `vehicules-2021.csv`

2. **Preprocess data**
   ```bash
   make run-preprocess
   # or
   python src/data/make_dataset.py
   ```
   This processes the raw data and creates train/test splits in `data/preprocessed/`.

3. **Train the baseline model**
   ```bash
   make run-train
   # or
   python src/models/train_model.py
   ```
   This trains a RandomForest classifier and saves it to `src/models/trained_model.joblib`.

4. **Make predictions**
   
   Using a JSON file:
   ```bash
   make run-predict-file FILE=src/models/test_features.json
   # or
   python src/models/predict_model.py src/models/test_features.json
   ```
   
   Or interactively (you'll be prompted to enter features manually):
   ```bash
   make run-predict
   # or
   python src/models/predict_model.py
   ```

## ğŸ› ï¸ Development Commands

The project includes a `Makefile` with common development tasks. Run `make help` to see all available commands:

### Dependency Management
- `make install` - Install project dependencies (production)
- `make install-dev` - Install project with development dependencies
- `make sync` - Sync dependencies from `pyproject.toml`
- `make update` - Update all dependencies to latest versions
- `make clean` - Remove build artifacts and cache files

### Code Quality
- `make lint` - Run linting with flake8
- `make format` - Format code with black and isort
- `make type-check` - Run type checking with mypy

### Testing
- `make test` - Run tests with pytest
- `make test-cov` - Run tests with coverage report

### Data Pipeline
- `make run-import` - Import raw data from S3
- `make run-preprocess` - Preprocess raw data
- `make run-train` - Train the baseline model
- `make run-predict` - Make predictions (interactive)
- `make run-predict-file FILE=path/to/file.json` - Make predictions from JSON file

### Setup
- `make setup` - Initial project setup (install dependencies)
- `make setup-venv` - Create virtual environment and install dependencies

## ğŸ”„ Workflow

### Current Workflow (Phase 1)

1. **Data Ingestion**: Download raw data from S3 using `import_raw_data.py`
2. **Data Preprocessing**: Transform and clean data using `make_dataset.py`
3. **Model Training**: Train RandomForest model using `train_model.py`
4. **Model Inference**: Make predictions using `predict_model.py`

### Target Workflow (Post Phase 1)

1. **Data Pipeline** (DVC): Automated data ingestion â†’ validation â†’ preprocessing
2. **Model Training** (MLflow): Config-driven training with experiment tracking
3. **Model Serving** (FastAPI): REST API for real-time predictions
4. **CI/CD** (GitHub Actions): Automated testing and deployment

## ğŸ‘¥ Team Structure

The project is designed for a 3-person team with clear separation of concerns:

### **Engineer A: Data & Pipeline Infrastructure**
- **Focus**: Getting data from raw to "model-ready"
- **Deliverables**: DVC pipelines, data validation (Pandera or manual), Dagshub integration
- **Primary Files**: `src/data/`, `dvc.yaml`, `params.yaml`

### **Engineer B: ML Modeling & Tracking**
- **Focus**: ML model development and experiment logging
- **Deliverables**: Training scripts, MLflow tracking, model registry management
- **Primary Files**: `src/models/`, `src/config/model_config.yaml`

### **Engineer C: API, Docker & CI/CD**
- **Focus**: Containerization, API development, and automation
- **Deliverables**: FastAPI application, Dockerfiles, GitHub Actions pipelines
- **Primary Files**: `src/api/`, `Dockerfile`, `.github/workflows/`

## ğŸ—ºï¸ Roadmap

### Phase 1: Foundations (Deadline: December 19th, 2024)
- âœ… Define project objectives and key metrics
- ğŸš§ Set up reproducible development environment (containerization, Docker)
- ğŸš§ Collect and preprocess data (ML Pipeline)
- ğŸš§ Build and evaluate baseline ML model, implement unit tests
- ğŸš§ Implement basic inference API

### Phase 2: Microservices, Tracking & Versioning (Deadline: January 16th, 2026)
- Set up experiment tracking with MLflow
- Implement data & model versioning (MLflow, DVC)
- Decompose application into microservices and design orchestration

### Phase 3: Orchestration & Deployment (Deadline: January 29th, 2026)
- Finalize end-to-end orchestration
- Create CI Pipeline (GitHub Actions: linter and others)
- Optimize and secure the API
- Implement scalability with Docker/Kubernetes

### Phase 4: Monitoring & Maintenance (Deadline: February 6th, 2026)
- Set up performance monitoring using Prometheus/Grafana
- Implement drift detection with Evidently
- Develop automated model and component updates
- Finalize technical documentation

**Final Presentation (Defence)**: February 9th, 2026

For detailed execution plans, see:
- [Phase 1 Plan](doc/Plan_Phase_01.md)
- [Roadmap](doc/Roadmap.md)

## ğŸ“ Development Guidelines

### Code Conventions

- All Python scripts must be run from the project root specifying the relative file path
- Use conventional commits (e.g., `feat:`, `fix:`, `docs:`)
- Follow PEP 8 style guidelines
- Use type hints where appropriate

### Branching Strategy

- Use feature branches: `feature/<ticket-id>-description`
- Pull requests require at least 1 approval and passing CI checks
- Main branch should always be in a deployable state

### Testing

- Unit tests should cover core functionality with >70% code coverage
- Tests should be written for `src/data/`, `src/models/`, and `src/api/` modules
- Run tests before submitting PRs: `make test` or `pytest`
- Generate coverage reports: `make test-cov`

### Dependency Management

- All dependencies are managed in `pyproject.toml`
- Use **UV** for installing dependencies: `make install-dev` or `uv pip install -e ".[dev]"`
- The legacy `requirements.txt` file is kept for reference but `pyproject.toml` is the source of truth
- Python version is specified in `.python-version` (for pyenv) and `.tool-versions` (for asdf)

## ğŸ¤ Contributing

1. **Set up your development environment**
   ```bash
   make install-dev  # Install dependencies with development tools
   ```

2. **Create a feature branch from `main`**
   ```bash
   git checkout -b feature/your-feature-name
   ```

3. **Make your changes following the development guidelines**

4. **Format and lint your code**
   ```bash
   make format    # Format with black and isort
   make lint      # Check with flake8
   make type-check # Type checking with mypy (if applicable)
   ```

5. **Write or update tests as needed**
   ```bash
   make test      # Run tests
   make test-cov  # Run tests with coverage
   ```

6. **Ensure all tests pass and code is properly formatted**
   - All tests must pass: `make test`
   - Code must be formatted: `make format`
   - No linting errors: `make lint`

7. **Submit a pull request with a clear description**

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Project based on the [cookiecutter data science project template](https://drivendata.github.io/cookiecutter-data-science/)
- Data sourced from French road accident databases

## ğŸ“š Additional Documentation

- [Initial README](doc/README_INITIAL.md) - Original project documentation
- [Phase 1 Plan](doc/Plan_Phase_01.md) - Detailed Phase 1 execution plan
- [Roadmap](doc/Roadmap.md) - Project roadmap and milestones

---

**Note**: This project is in active development. The structure and workflows are being refined as we progress through Phase 1. For the most up-to-date information, refer to the documentation in the `doc/` directory.

