# ğŸš¦ MLOps Road Accident Prediction

A containerized MLOps project for predicting road accidents using machine learning. This project implements a complete MLOps workflow from data ingestion to model serving, with a focus on reproducibility, versioning, and best practices.

## ğŸ“‹ Table of Contents

- [Project Overview](#project-overview)
- [Current State](#current-state)
- [Project Structure](#project-structure)
- [Technology Stack](#technology-stack)
- [Getting Started](#getting-started)
- [Workflow](#workflow)
- [Team Structure](#team-structure)
- [Roadmap](#roadmap)
- [Contributing](#contributing)
- [License](#license)

## ğŸ¯ Project Overview

This project is an MLOps implementation for road accident prediction, designed as a learning and production-ready template. The system processes road accident data from multiple sources, trains machine learning models to predict accident severity, and serves predictions through a REST API.

### Objectives

- **Reproducibility**: Ensure all experiments and workflows can be reproduced across different environments
- **Versioning**: Track data, models, and experiments using DVC and MLflow
- **Containerization**: Isolate environments using Docker for consistent deployments
- **Automation**: Implement CI/CD pipelines for testing, linting, and deployment
- **Monitoring**: Track model performance and detect data drift in production

### Success Metrics

- **Model Performance**: F1 Score, Precision, and Recall for accident severity classification
- **Baseline Model**: RandomForest with default parameters as initial benchmark
- **Minimum Performance Threshold**: To be defined based on baseline results

## ğŸ“Š Current State

**Phase**: Phase 1 - Foundations (Deadline: December 19th)

### âœ… Completed

- Project structure based on cookiecutter-data-science template
- Reproducible Python environment setup with `pyproject.toml` and UV
- Development automation with Makefile
- Python version management (`.python-version`, `.tool-versions`)

- Initial data exploration notebook

### ğŸš§ In Progress

- Containerization with Docker
- Data import pipeline (`import_raw_data.py`)
- Data preprocessing pipeline (`make_dataset.py`)
- Baseline model training (`train_model.py`)
- Model prediction script (`predict_model.py`)
- DVC + Dagshub integration for data versioning
- MLflow integration for experiment tracking
- FastAPI service for model serving
- Unit testing suite
- CI/CD pipeline with GitHub Actions

### ğŸ“ Planned

See [Roadmap](#roadmap) section for detailed phase breakdown.

## ğŸ“ Project Structure

```
MLOps_accidents/
â”œâ”€â”€ .github/workflows/         # CI/CD pipelines (to be implemented)
â”‚   â”œâ”€â”€ lint.yaml
â”‚   â””â”€â”€ test.yaml
â”œâ”€â”€ data/                      # Data directory (created at runtime)
â”‚   â”œâ”€â”€ external/              # Data from third party sources
â”‚   â”œâ”€â”€ interim/               # Intermediate data that has been transformed
â”‚   â”œâ”€â”€ processed/             # The final, canonical data sets for modeling
â”‚   â””â”€â”€ raw/                   # The original, immutable data dump
â”œâ”€â”€ doc/                       # Project documentation
â”‚   â”œâ”€â”€ README_INITIAL.md      # Initial project documentation
â”‚   â”œâ”€â”€ Plan_Phase_01.md       # Detailed Phase 1 execution plan
â”‚   â””â”€â”€ Roadmap.md             # Project roadmap and milestones
â”œâ”€â”€ logs/                      # Logs from training and predicting
â”œâ”€â”€ models/                    # Trained and serialized models
â”œâ”€â”€ notebooks/                 # Jupyter notebooks
â”‚   â””â”€â”€ 1.0-ldj-initial-data-exploration.ipynb
â”œâ”€â”€ references/                # Data dictionaries, manuals, and explanatory materials
â”œâ”€â”€ reports/                   # Generated analysis as HTML, PDF, LaTeX, etc.
â”‚   â””â”€â”€ figures/               # Generated graphics and figures
â”œâ”€â”€ src/                       # Source code for use in this project
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config/                # Configuration files (YAML configs)
â”‚   â”œâ”€â”€ data/                  # Scripts to download or generate data
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ check_structure.py
â”‚   â”‚   â”œâ”€â”€ import_raw_data.py # Downloads data from S3
â”‚   â”‚   â””â”€â”€ make_dataset.py    # Preprocesses raw data
â”‚   â”œâ”€â”€ features/              # Scripts to turn raw data into features
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ build_features.py
â”‚   â”œâ”€â”€ models/                # Scripts to train models and make predictions
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ predict_model.py   # Model inference script
â”‚   â”‚   â”œâ”€â”€ train_model.py     # Model training script
â”‚   â”‚   â””â”€â”€ test_features.json # Example features for testing
â”‚   â””â”€â”€ visualization/         # Scripts to create visualizations
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ visualize.py
â”œâ”€â”€ tests/                     # Pytest suite (to be implemented)
â”‚   â”œâ”€â”€ test_data.py
â”‚   â”œâ”€â”€ test_models.py
â”‚   â””â”€â”€ test_api.py
â”œâ”€â”€ .dockerignore              # Docker ignore file (to be created)
â”œâ”€â”€ .python-version            # Python version for pyenv
â”œâ”€â”€ .tool-versions             # Python version for asdf
â”œâ”€â”€ Dockerfile                 # Multi-stage Dockerfile (to be created)
â”œâ”€â”€ docker-compose.yaml        # Local orchestration (to be created)
â”œâ”€â”€ dvc.yaml                   # DVC pipeline definition (to be created)
â”œâ”€â”€ LICENSE                    # MIT License
â”œâ”€â”€ Makefile                   # Development commands and automation
â”œâ”€â”€ pyproject.toml             # Python project configuration and dependencies
â”œâ”€â”€ requirements.txt           # Python dependencies (legacy, use pyproject.toml)
â”œâ”€â”€ setup.py                   # Package setup configuration (legacy)
â””â”€â”€ README.md                  # This file
```

## ğŸ› ï¸ Technology Stack

| Component | Technology | Role |
| :--- | :--- | :--- |
| **Data Versioning** | **DVC + Dagshub** | Versioning raw data and artifacts without AWS |
| **Experiment Tracking** | **MLflow (Dagshub)** | Tracking metrics, parameters, and models |
| **Pipeline Stages** | **Docker Containers** | Isolated environments for Data, Training, and API |
| **Model Serving** | **FastAPI** | REST API for real-time accident prediction |
| **CI/CD** | **GitHub Actions** | Automated testing, linting, and image building |
| **Machine Learning** | **scikit-learn** | Model training and evaluation |
| **Data Processing** | **pandas, numpy** | Data manipulation and preprocessing |
| **Testing** | **pytest** | Unit and integration testing |
| **Code Quality** | **black, flake8, isort** | Code formatting and linting |
| **Dependency Management** | **UV** | Fast Python package installer and resolver |
| **Build System** | **setuptools** | Package building and distribution |

## ğŸš€ Getting Started

### Prerequisites

- **Python 3.8+** (3.11 recommended - see `.python-version`)
- **UV** - Fast Python package installer ([Install](https://github.com/astral-sh/uv): `curl -LsSf https://astral.sh/uv/install.sh | sh`)
- **DVC** - Installed automatically with dependencies
- **Docker & Docker Compose** - For containerized workflow
- **Git** & **Make** - Usually pre-installed
- **Dagshub account** - [Sign up](https://dagshub.com) for data versioning

### Quick Start

1. **Clone and setup Python** (optional)
   ```bash
   git clone https://github.com/chrmei/MLOps_accidents.git
   cd MLOps_accidents
   # Using pyenv: pyenv install 3.11.0 && pyenv local 3.11.0
   # Using asdf: asdf install python 3.11.0 && asdf reshim python
   ```

2. **Install dependencies**
   ```bash
   make install-dev  # Installs project + dev dependencies (pytest, black, isort, mypy, etc.)
   # Alternative: uv pip install -e ".[dev]"
   ```

3. **Verify installation**
   ```bash
   python --version && make help
   ```

5. **Set up DVC and Dagshub (Optional but recommended)**
   
   DVC is used for data versioning. To set it up:
   
   ```bash
   # Step 1: Create .env file from template
   cp .env.example .env
   
   # Step 2: MANUALLY EDIT .env file with your Dagshub credentials:
   #   - DAGSHUB_USERNAME: Your Dagshub username
   #   - DAGSHUB_TOKEN: Get from https://dagshub.com/user/settings/tokens
   #   - DAGSHUB_REPO: Your repository (e.g., chrmei/MLOps_accidents)
   # 
   # IMPORTANT: You must manually edit .env before running the next commands!
   
   # Step 3: Initialize DVC and configure remote using Makefile
   make dvc-init
   make dvc-setup-remote
   ```
   
   **Important Notes**:
   - The `.env` file must be **manually edited** with your credentials before running `make dvc-setup-remote`
   - The `.env` file is gitignored and will never be committed
   - Each team member should create their own `.env` file with their personal Dagshub credentials
   - See [DVC Commands](#dvc-data-version-control) section for all available DVC commands

### Run the Pipeline

```bash
# 1. Import raw data (downloads 4 CSV files from AWS S3)
make run-import

# 2. Preprocess data (creates train/test splits in data/preprocessed/)
make run-preprocess

# 3. Train baseline model (saves to src/models/trained_model.joblib)
make run-train

# 4. Make predictions
make run-predict                    # Interactive mode
make run-predict-file FILE=path     # From JSON file
```

## ğŸ› ï¸ Development Commands

Run `make help` to see all commands. Key commands:

**Setup & Dependencies**
- `make install-dev` - Install with dev dependencies
- `make setup-venv` - Create venv and install dependencies
- `make clean` - Remove build artifacts

**Code Quality**
- `make format` - Format with black & isort
- `make lint` - Lint with flake8
- `make type-check` - Type checking with mypy

**Testing**
- `make test` - Run pytest
- `make test-cov` - Run with coverage report

**Data Pipeline**
- `make run-import` - Import raw data
- `make run-preprocess` - Preprocess data
- `make run-train` - Train model
- `make run-predict` - Interactive predictions
- `make run-predict-file FILE=path` - Predictions from JSON

**DVC (Data Version Control)**
- `make dvc-init` - Initialize DVC
- `make dvc-setup-remote` - Configure Dagshub remote (**requires manually edited .env**)
- `make dvc-status` / `dvc-push` / `dvc-pull` / `dvc-repro`

> **Important**: For `make dvc-setup-remote`, you must first manually edit `.env` with your Dagshub credentials (copy from `.env.example`).

## ğŸ”„ Workflow

**Current (Phase 1)**: Data Ingestion â†’ Preprocessing â†’ Training â†’ Inference  
**Target (Post Phase 1)**: DVC Pipeline â†’ MLflow Tracking â†’ FastAPI Serving â†’ CI/CD

## ğŸ‘¥ Team Structure

The project is designed for a 3-person team with clear separation of concerns:

### **Engineer A: Data & Pipeline Infrastructure**
- **Focus**: Getting data from raw to "model-ready"
- **Deliverables**: DVC pipelines, data validation (Pandera or manual), Dagshub integration
- **Primary Files**: `src/data/`, `dvc.yaml`, `params.yaml`

### **Engineer B: ML Modeling & Tracking**
- **Focus**: ML model development and experiment logging
- **Deliverables**: Training scripts, MLflow tracking, model registry management
- **Primary Files**: `src/models/`, `src/config/model_config.yaml`

### **Engineer C: API, Docker & CI/CD**
- **Focus**: Containerization, API development, and automation
- **Deliverables**: FastAPI application, Dockerfiles, GitHub Actions pipelines
- **Primary Files**: `src/api/`, `Dockerfile`, `.github/workflows/`

## ğŸ—ºï¸ Roadmap

### Phase 1: Foundations (Deadline: December 19th, 2024)
- âœ… Define project objectives and key metrics
- ğŸš§ Set up reproducible development environment (containerization, Docker)
- ğŸš§ Collect and preprocess data (ML Pipeline)
- ğŸš§ Build and evaluate baseline ML model, implement unit tests
- ğŸš§ Implement basic inference API

### Phase 2: Microservices, Tracking & Versioning (Deadline: January 16th, 2026)
- Set up experiment tracking with MLflow
- Implement data & model versioning (MLflow, DVC)
- Decompose application into microservices and design orchestration

### Phase 3: Orchestration & Deployment (Deadline: January 29th, 2026)
- Finalize end-to-end orchestration
- Create CI Pipeline (GitHub Actions: linter and others)
- Optimize and secure the API
- Implement scalability with Docker/Kubernetes

### Phase 4: Monitoring & Maintenance (Deadline: February 6th, 2026)
- Set up performance monitoring using Prometheus/Grafana
- Implement drift detection with Evidently
- Develop automated model and component updates
- Finalize technical documentation

**Final Presentation (Defence)**: February 9th, 2026

For detailed execution plans, see:
- [Phase 1 Plan](doc/Plan_Phase_01.md)
- [Roadmap](doc/Roadmap.md)

## ğŸ“ Development Guidelines

**Code**: Run scripts from project root, use conventional commits (`feat:`, `fix:`), follow PEP 8, add type hints  
**Branches**: `feature/<ticket-id>-description`, PRs require approval + passing CI  
**Testing**: >70% coverage for `src/data/`, `src/models/`, `src/api/`; run `make test` before PRs  
**Dependencies**: Managed in `pyproject.toml` via UV (`make install-dev`); Python version in `.python-version`

## ğŸ¤ Contributing

1. Setup: `make install-dev`
2. Branch: `git checkout -b feature/your-feature-name`
3. Develop: Make changes following guidelines
4. Quality: `make format && make lint && make type-check`
5. Test: `make test` (coverage: `make test-cov`)
6. PR: Ensure all checks pass, submit with clear description

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Project based on the [cookiecutter data science project template](https://drivendata.github.io/cookiecutter-data-science/)
- Data sourced from French road accident databases

## ğŸ“š Additional Documentation

- [Initial README](doc/README_INITIAL.md) - Original project documentation
- [Phase 1 Plan](doc/Plan_Phase_01.md) - Detailed Phase 1 execution plan
- [Roadmap](doc/Roadmap.md) - Project roadmap and milestones

---

**Note**: This project is in active development. The structure and workflows are being refined as we progress through Phase 1. For the most up-to-date information, refer to the documentation in the `doc/` directory.

